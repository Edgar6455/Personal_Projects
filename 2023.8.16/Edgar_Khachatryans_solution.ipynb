{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Download and unzip the English texts collection, which contains over 100,000 documents. The documents will be saved in the /content/enwiki directory"
      ],
      "metadata": {
        "id": "VPsvgTKmIo9h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDSsSh0BU8iS"
      },
      "outputs": [],
      "source": [
        "!gdown https://drive.google.com/uc?id=1wzj7AXu_UmwrMc5Tl1_acMKbYdKWaXMC\n",
        "!unzip /content/enwiki.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section A\n",
        "\n",
        "Simple code for searching phrases"
      ],
      "metadata": {
        "id": "Uzz1ujuDw9vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This code block prepares documents for further searching\n",
        "\n",
        "from typing import List, Dict\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "def read_text_fragments(file_path: str) -> List[str]:\n",
        "    with open(file_path, \"r\") as f:\n",
        "        fragments = f.readlines()\n",
        "    # Remove leading and trailing spaces and new line characters before returning text fragments\n",
        "    return [fragment.strip() for fragment in fragments if fragment.strip()]\n",
        "\n",
        "def extract_and_combine_text_fragments(folder_path: str) -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    This function iterates over all files in the folder and saves their fragments in a dictionary,\n",
        "    Returns:\n",
        "        A dictionary, where the dictionary key is the file name, and its value is a list of all fragments in that file.\n",
        "    Example:\n",
        "        texts_fragments_dict = {\n",
        "              \"file_1.txt\": [\"this is an example of some fragment\", \"this is an example of another fragment\"],\n",
        "              \"file_2.txt\": [\"this is another example\"],\n",
        "              ...\n",
        "        }\n",
        "    \"\"\"\n",
        "\n",
        "    texts_fragments_dict = {}\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        # Extract text file fragments\n",
        "        file_fragments = read_text_fragments(file_path)\n",
        "        # Add file name and its value to texts_fragments_dict\n",
        "        texts_fragments_dict[file_name] = file_fragments\n",
        "    return texts_fragments_dict\n",
        "\n",
        "# Extract all documents fragments from the specified folder\n",
        "folder_path = \"/content/enwiki\"\n",
        "texts_fragments_dict = extract_and_combine_text_fragments(folder_path=folder_path)"
      ],
      "metadata": {
        "id": "nNiQcWYG2qeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that searches similar phrases\n",
        "def search_similar_phrases(phrase: str, texts_fragments: Dict[str, List[str]]) -> List[List[str]]:\n",
        "    \"\"\"\n",
        "    For the given phrase, search for similar phrases in the texts_fragments dict.\n",
        "    Returns:\n",
        "        A list containing information about in which file the phrase is written and the corresponding fragment.\n",
        "        If nothing is found, it returns an empty list.\n",
        "    Example:\n",
        "        search_results = [\n",
        "            [\"file_1.txt\", \"this is an example of some fragment\"],\n",
        "            [\"file_1.txt\", \"this is an example of another fragment\"],\n",
        "            [\"file_2.txt\", \"this is another example\"]\n",
        "            ...\n",
        "        ]\n",
        "    \"\"\"\n",
        "    search_results = []\n",
        "    # Iterate over all files\n",
        "    for file_name in texts_fragments:\n",
        "        # Iterate over file fragments\n",
        "        for fragment in texts_fragments[file_name]:\n",
        "            # Check if the phrase is in our fragment, then save it in the search_results list\n",
        "            if phrase in fragment:\n",
        "                search_results.append([file_name, fragment])\n",
        "\n",
        "    return search_results"
      ],
      "metadata": {
        "id": "0ym1bWUUEdqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of searching\n",
        "phrase = \"Berisha is elected chairman\"\n",
        "search_results = search_similar_phrases(phrase, texts_fragments_dict)\n",
        "print(search_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-25z27aGHyZ",
        "outputId": "4dc181c7-0921-476b-80c8-4c3fe57ddf25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['17919119.txt', 'Berisha is elected chairman of the Democratic Party.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This algorithm works slowly; if we have, for example, a million texts, it can take minutes to return an answer.\n",
        "\n",
        "Besides this, the proposed method only finds identical texts."
      ],
      "metadata": {
        "id": "kieOz_A5GaqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. If we delete some of the middle words, we will receive an empty result\n",
        "phrase = \"Berisha elected chairman\"\n",
        "search_results = search_similar_phrases(phrase, texts_fragments_dict)\n",
        "print(search_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT3w5MmkGZmV",
        "outputId": "0a839f48-4f62-47c0-c4e4-6e9041c7338d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Or if we delete some word ending\n",
        "phrase = \"Berisha is elect chairman\"\n",
        "search_results = search_similar_phrases(phrase, texts_fragments_dict)\n",
        "print(search_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACapfPnyGuGe",
        "outputId": "3393954e-0491-4371-9b5e-d51ef7296b5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Or if we make words lowercase\n",
        "phrase = \"berisha is elected chairman\"\n",
        "search_results = search_similar_phrases(phrase, texts_fragments_dict)\n",
        "print(search_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LRY7VotGuJX",
        "outputId": "65a23582-8aaa-4e93-8fb5-6777289336d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Or if we change some similar meaning words, \"elected\" -> \"appointed\"\n",
        "phrase = \"Berisha is appointed chairman\"\n",
        "search_results = search_similar_phrases(phrase, texts_fragments_dict)\n",
        "print(search_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnHSNgiyGuMU",
        "outputId": "caac1c4e-70b1-4e7b-c6a4-da3c6344fbeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section B\n",
        "\n",
        "Research and implement a search algorithm that overcomes these limitations and performs efficiently and rapidly on a vast collection of documents. Additionally, you need to explore other potential problems that might occur during the search process and propose solutions to avoid them."
      ],
      "metadata": {
        "id": "GK6qC8znMp0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately, there is an issue with the RAM limitation in google colab. So it's recomended to run the code for a part of documents. Here is the 10% (10.000 files) of all the documents."
      ],
      "metadata": {
        "id": "gpJNlmGFLyKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1_2sN1ZBL2sxvZn46ebBDvoLRL4AenquB&export=download\n",
        "!unzip /content/enwiki.zip"
      ],
      "metadata": {
        "id": "XykhZ6OLL1Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the text files preprocessing part.\n",
        "\n",
        "Note: You can uncomment the line \"line_list = perform_stemming(line_list)\" in the function \"normalize_line_and_split\", so you will get results with modified word endings. However, preprocessing will consume significantly more time."
      ],
      "metadata": {
        "id": "PPddXN1LrFu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Tuple, Set\n",
        "from collections import defaultdict  # Provides a default value for keys that don't exist in the dictionary\n",
        "import linecache                     # To get a line from a text document\n",
        "import os                            # Read documents\n",
        "import re                            # Regular expression to simplify some functions\n",
        "import time                          # To measure searching time\n",
        "import nltk                          # ...\n",
        "from nltk.stem import PorterStemmer  # To cut word endings\n",
        "\n",
        "\n",
        "def perform_stemming(word_list: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    This function iterates over all tokens in a list, performs stemming on each.\n",
        "    Returns:\n",
        "        A list of strings, where each token is stemmed.\n",
        "    Example:\n",
        "        stemmed_words = [\"run\", \"write\", \"histor\", ...]\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in word_list]\n",
        "    return stemmed_words\n",
        "\n",
        "\n",
        "# Function that splitting a line into a list of tokens\n",
        "def split_line(input_string: str) -> List[str]:\n",
        "    return re.split(r'\\s+', input_string)\n",
        "\n",
        "\n",
        "# Function that removes punctuation from a line\n",
        "def remove_punctuation(input_string: str) -> str:\n",
        "    return re.sub(r'[^\\w\\s]', '', input_string)\n",
        "\n",
        "\n",
        "def normalize_line_and_split(input_line: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    This function tokenizes the given line.\n",
        "    Note: I could write a function, that does everything here manually(except stemming). That shall decrease preprocessing time.\n",
        "    Returns:\n",
        "        A list of strings, where each token is stripped, lowercased, punctuation removed and stemmed(optional).\n",
        "    Example:\n",
        "        input_line = \" (objects that are proved to exist, but \"\n",
        "        line_list = [\"objects\", \"that\", \"are\", \"proven\", \"to\", \"exist\", \"but\"] # Not stemmed\n",
        "        line_list = [\"object\", \"that\", \"are\", \"prov\", \"to\", \"exist\", \"but\"]   # Stemmed\n",
        "    \"\"\"\n",
        "    input_line = input_line.strip()\n",
        "    input_line = remove_punctuation(input_line)\n",
        "    input_line = input_line.lower()\n",
        "    line_list = split_line(input_line)\n",
        "    #line_list = perform_stemming(line_list)\n",
        "    return line_list\n",
        "\n",
        "\n",
        "# Reads file content via given path and Stores its content to a list of strings, where every string is a line from the file\n",
        "def read_given_file(file_path: str) -> List[str]:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return f.readlines()\n",
        "\n",
        "\n",
        "def append_file_content_to_dictionary(lines: List[str], file_name: str):\n",
        "    \"\"\"\n",
        "    This function iterates over file content, tokenizes it, and appends tokens to a data structure, holding all tokens and their position.\n",
        "    In token_dictionary key is a token(string) and value is a set of tuples, where elements of tuple are file name(string) and line id(int).\n",
        "    Example:\n",
        "        token_dictionary = {\n",
        "              \"example\": [(\"file_name_1.txt\", 14), (\"file_name_1.txt\", 23), (\"file_name_2.txt\", 57), ...],\n",
        "              \"another\": [(\"file_name_1.txt\", 63), (\"file_name_3.txt\", 35), ...],\n",
        "              ...\n",
        "        }\n",
        "    \"\"\"\n",
        "    for line_id in range(len(lines)):\n",
        "        if lines[line_id].strip():\n",
        "            token_list = normalize_line_and_split(lines[line_id])\n",
        "            for token_id in range(len(token_list)):\n",
        "                token_dictionary[token_list[token_id]].add((file_name, line_id + 1))\n",
        "\n",
        "\n",
        "# This function iterates over all files, reads their content, tokenizes them and saves their tokens in a dictionary,\n",
        "def extract_and_combine_text_files():\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        lines = read_given_file(file_path)\n",
        "        append_file_content_to_dictionary(lines, file_name)\n",
        "\n",
        "\n",
        "token_dictionary = defaultdict(set)\n",
        "folder_path = \"/content/enwiki\"\n",
        "\n",
        "# Download the \"punkt\" dataset from the NLTK library for stemming.\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "print(\"\\nPreprocessing files...\")\n",
        "extract_and_combine_text_files()\n",
        "print(\"Ended!\")"
      ],
      "metadata": {
        "id": "k0DJqgGmdCs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the searching logic functions."
      ],
      "metadata": {
        "id": "fFKNT_P7wCOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_search_results_intersection(phrase_tokens: List[str]) -> Set[Tuple[str, int]]:\n",
        "    \"\"\"\n",
        "    Finds lines, where all the tokens from a searching phrase appear.\n",
        "    Returns:\n",
        "        A set of tuples, where tuples are values from the token_dictionary (file name(string) and line id(int)).\n",
        "    Example:\n",
        "        intersection_result = {(file_name_1.txt\", 25), (file_name_3.txt\", 72), ...}\n",
        "    \"\"\"\n",
        "    intersection_result = Set[Tuple[str, int]]\n",
        "    # Iterate over all phrase tokens\n",
        "    for token_id in range(len(phrase_tokens)):\n",
        "        # If it's first token, initialize result by it's elements\n",
        "        if token_id == 0:\n",
        "            intersection_result = token_dictionary[phrase_tokens[token_id]]\n",
        "        # Intersect result with token_id-th token's value\n",
        "        else:\n",
        "            intersection_result = intersection_result.intersection(token_dictionary[phrase_tokens[token_id]])\n",
        "    return intersection_result\n",
        "\n",
        "\n",
        "# Function that receives file name and line number and returns its content\n",
        "def get_line_content(file_name: str, line_number: int) -> str:\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    return linecache.getline(file_path, line_number)[:-1]\n",
        "\n",
        "\n",
        "def get_matches_from_line(line_tokens_list: List[str], phrase_tokens: List[str]) -> List[int]:\n",
        "    \"\"\"\n",
        "    Function, that find given phrase on the given line.\n",
        "    Note: There is a version of this function below, that enables results with a middle word missing from the searched phrase\n",
        "    Returns:\n",
        "        A list of positions (token_id (int)), where phrase appears on the line.\n",
        "    Example:\n",
        "        result = [10, 33, ...]\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    # Iterate over all tokens in the given line\n",
        "    for token_id in range(len(line_tokens_list)):\n",
        "        # Checks if the token from line matches with the first token of the searching phrase\n",
        "        if line_tokens_list[token_id] != phrase_tokens[0]:\n",
        "            continue\n",
        "        # This logic block checks if the next tokens of the line match with the tokens of the phrase\n",
        "        exact_match = True\n",
        "        # Iterate over phrase tokens and check if the corresponding token of the line matches with it\n",
        "        for token_from_phrase_id in range(1, len(phrase_tokens)):\n",
        "            # This condition prevents index going out of bounds\n",
        "            if token_id + token_from_phrase_id >= len(line_tokens_list):\n",
        "                exact_match = False\n",
        "                break\n",
        "            if phrase_tokens[token_from_phrase_id] != line_tokens_list[token_id + token_from_phrase_id]:\n",
        "                exact_match = False\n",
        "                break\n",
        "        # If none of conditions above happen, then it's a match\n",
        "        if exact_match:\n",
        "            # Append token position from the line, to the list\n",
        "            result.append(token_id)\n",
        "    return result\n",
        "\n",
        "\n",
        "# Global variable to measure time spent on a search request\n",
        "time_taken = 0.0\n",
        "\n",
        "\n",
        "def search_similar_phrases(phrase: str) -> List[tuple[str, int, int, str]]:\n",
        "    \"\"\"\n",
        "    Function that combines functions above to find the phrase positions in text documents\n",
        "    Returns:\n",
        "        A list of tuples, where elements of it are file name, line number, position on the line, and line content.\n",
        "    Example:\n",
        "        final_result = [\n",
        "              (\"file_name_1.txt\", 14, 142, \"Line content from file_name_1.txt and line 14\"),\n",
        "              (\"file_name_3.txt\", 66, 20, \"Line content from file_name_3.txt and line 66\"),\n",
        "              ...\n",
        "        ]\n",
        "    \"\"\"\n",
        "    global time_taken\n",
        "    # Start measuring time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Tokenize phrase\n",
        "    phrase_tokens = normalize_line_and_split(phrase)\n",
        "    final_result = []\n",
        "\n",
        "    # Intersects all phrase tokens positions, to determine lines, where phrase tokens appeare together\n",
        "    intersection_result = get_search_results_intersection(phrase_tokens)\n",
        "\n",
        "    # This code block appends to the final_result exact position where phrase appears in text documents\n",
        "    for match in intersection_result:\n",
        "        # Get file name and line number, where all tokens from phrase appear together\n",
        "        file_name = match[0]\n",
        "        line_number = int(match[1])\n",
        "        # Get line content, tokenize, then get positions (if there are any), where phrase tokens appear consequently\n",
        "        line_content = get_line_content(file_name, line_number)\n",
        "        line_tokens_list = normalize_line_and_split(line_content)\n",
        "        match_list = get_matches_from_line(line_tokens_list, phrase_tokens)\n",
        "        # Append that positions to the final result\n",
        "        for match_id in range(len(match_list)):\n",
        "            token_number = match_list[match_id] + 1\n",
        "            final_result.append((file_name, line_number, token_number, line_content))\n",
        "\n",
        "    # Stop the timer\n",
        "    end_time = time.time()\n",
        "    time_taken = end_time - start_time\n",
        "\n",
        "    return final_result\n",
        "\n",
        "\n",
        "# A function to print results found\n",
        "def print_results(final_result: List[tuple[str, int, int, str]]):\n",
        "    print()\n",
        "    formatted_time = \"{:.5f}\".format(time_taken)\n",
        "    # Case, when nothing found\n",
        "    if len(final_result) == 0:\n",
        "        print(\"Not found! \" + formatted_time + \" seconds spent.\")\n",
        "        print()\n",
        "        return\n",
        "    # Iterate over results and print it\n",
        "    for result in final_result:\n",
        "        file_name = result[0]\n",
        "        line_number = result[1]\n",
        "        word_number = result[2]\n",
        "        line_content = result[3]\n",
        "        print(file_name + \", line \" + str(line_number) + \", word \" + str(word_number) + \": \" + line_content)\n",
        "    print()\n",
        "    print(f\"Found \" + str(len(final_result)) + \" results in \" + formatted_time + \" seconds.\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "D5ZmOnjflmAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_results(final_result: List[tuple[str, int, int, str]]):\n",
        "    print()\n",
        "    formatted_time = \"{:.5f}\".format(time_taken)\n",
        "    print(f\"Found \" + str(len(final_result)) + \" results in \" + formatted_time + \" seconds.\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "a3M0AICPx-Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you can search for a specific phrase.\n",
        "\n",
        "Note: If there are many lines of result, and google colab is not showing it properly, to get searching time and results count, you can use function above."
      ],
      "metadata": {
        "id": "h2lPlSYAwZWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = input(\"Enter phrase to search: \")\n",
        "\n",
        "results = search_similar_phrases(user_input)\n",
        "print_results(results)"
      ],
      "metadata": {
        "id": "YFxEGSOKlsu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can run this code fragment to enable search results, including deleted middle words from the phrase being searched."
      ],
      "metadata": {
        "id": "MIAPRJYpcb9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_matches_from_line(line_tokens_list: List[str], phrase_tokens: List[str]) -> List[int]:\n",
        "    result = []\n",
        "    for token_number in range(len(line_tokens_list)):\n",
        "        if line_tokens_list[token_number] != phrase_tokens[0]:\n",
        "            continue\n",
        "        exact_match = True\n",
        "        # Everything above here is not changed\n",
        "        # Bool variable to ignore 1 middle word missing from phrase searched on the line\n",
        "        middle_word_missing = False\n",
        "        phrase_token_index = 1\n",
        "        # Iterate over line tokens and check if the corresponding token of the phrase matches with it\n",
        "        for current_token_number in range(token_number + 1, token_number + len(phrase_tokens)):\n",
        "            # Case, where phrase token and line tokens are not the corresponding\n",
        "            if phrase_tokens[phrase_token_index] != line_tokens_list[current_token_number]:\n",
        "                # Case, where there is already one missing token in phrase\n",
        "                if middle_word_missing:\n",
        "                    exact_match = False\n",
        "                    break\n",
        "                # If it's first time, then don't change phrase token and check for the next line token\n",
        "                else:\n",
        "                    phrase_token_index -= 1\n",
        "                    middle_word_missing = True\n",
        "            phrase_token_index += 1\n",
        "        # Again, no changes here\n",
        "        if exact_match:\n",
        "            result.append(token_number)\n",
        "    return result"
      ],
      "metadata": {
        "id": "hpWZYf1B7v1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is an example of enabled/disabled missing middle word to try."
      ],
      "metadata": {
        "id": "YnvARLWeh9fV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = search_similar_phrases(\"media outlets suggested displaying\")\n",
        "print_results(results)"
      ],
      "metadata": {
        "id": "i7S3bl5LiRh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Advantages:**"
      ],
      "metadata": {
        "id": "JeXYDjyeeil_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. If we search for a concrete phrase, results will be found very quickly, which is a realistic case of using this algorithm\n",
        "#    missing middle word - disabled, stemming - disabled\n",
        "\n",
        "results = search_similar_phrases(\"provide nonconstructive proofs\")\n",
        "print_results(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbTWe4jOfMQO",
        "outputId": "7dd61a56-3c34-475d-856f-e692d9a1fa3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "840.txt, line 52, word 13: As discussed above, in ZFC, the axiom of choice is able to provide \"nonconstructive proofs\" in which the existence of an object is proved although no explicit example is constructed. ZFC, however, is still formalized in classical logic. The axiom of choice has also been thoroughly studied in the context of constructive mathematics, where non-classical logic is employed. The status of the axiom of choice varies between different varieties of constructive mathematics.\n",
            "\n",
            "Found 1 results in 0.00012 seconds.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. If we search without punctuation, using uppercase/lowercase characters, which are not the same as in text, there is no difference: algorithm will find it\n",
        "#    missing middle word - disabled, stemming - disabled\n",
        "\n",
        "results = search_similar_phrases(\"watercress nasturtium officinale is\")\n",
        "print_results(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TF55drSJhmmU",
        "outputId": "58282586-fd63-4fa5-eac9-aa74ca1b8c49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "15374.txt, line 52, word 1: Watercress (\"Nasturtium Officinale\") is a rapidly growing aquatic or semi aquatic perennial plant. It contains health promoting phytochemicals endowed in therapeutic properties. Because chemical agents do not provide efficient microbial reductions, watercress has been tested with gamma irradiation treatment in order to improve both safety and the shelf life of the product. It is traditionally used on horticultural products to prevent sprouting and post-packaging contamination, delay post-harvest ripening, maturation and senescence.\n",
            "\n",
            "Found 1 results in 0.00032 seconds.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1. If we search a phrase with an unstripped content, or search for a number, when format is not corresponding original, that's also shall be found\n",
        "#    missing middle word - disabled, stemming - disabled\n",
        "\n",
        "results = search_similar_phrases(\"  451.000        students   \")\n",
        "print_results(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iY9pxZliv2O",
        "outputId": "c4edea4f-36d6-4307-af60-0c1c8b73ffd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "19226.txt, line 106, word 7: There are almost half a million (451,000) students enrolled in electronics engineering programs with an additional 114,000 electronics engineers entering the Mexican workforce each year and Mexico had over half a million (580,000) certified electronic engineering professionals employed in 2007. From the late 1990s, the Mexican electronics industry began to shift away from simple line assembly to more advanced work such as research, design, and the manufacture of advanced electronics systems such as LCD panels, semiconductors, printed circuit boards, microelectronics, microprocessors, chipsets and heavy electronic industrial equipment and in 2006 the number of certified engineers being graduated annually in Mexico surpassed that of the United States. Many Korean, Japanese and American appliances sold in the US are actually of Mexican design and origin but sold under the OEM's client names. In 2008 one out of every four consumer appliances sold in the United States was of Mexican design.\n",
            "\n",
            "Found 1 results in 0.00020 seconds.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. It's possible to find results, even if there is a 1 missing word in a phrase\n",
        "#    missing middle word - enabled, stemming - disabled\n",
        "\n",
        "results = search_similar_phrases(\"gradually become hospitable\")\n",
        "print_results(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gGdGo-hjtMN",
        "outputId": "c31c5264-983a-4915-d4ce-3f02a9e97ec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "9813.txt, line 45, word 10: It has also been suggested that the oceans have gradually become more hospitable to life over the last 500 million years, and thus less vulnerable to mass extinctions, but susceptibility to extinction at a taxonomic level does not appear to make mass extinctions more or less probable.\n",
            "\n",
            "Found 1 results in 0.00065 seconds.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. It's possible to find results, where word endings are different\n",
        "#    missing middle word - disabled, stemming - enabled\n",
        "\n",
        "results = search_similar_phrases(\"period of increased geomagnetic reversal\")\n",
        "print_results(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWY5Ad5ou_Jh",
        "outputId": "d3afe6cf-6e08-4fcc-f513-d585be8dd3d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "9813.txt, line 109, word 5: One theory is that periods of increased geomagnetic reversals will weaken Earth's magnetic field long enough to expose the atmosphere to the solar winds, causing oxygen ions to escape the atmosphere in a rate increased by 3–4 orders, resulting in a disastrous decrease in oxygen.\n",
            "\n",
            "Found 1 results in 0.01931 seconds.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Disadvantages:**"
      ],
      "metadata": {
        "id": "lbBuw9fD9Zmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. There are cases, where punctuation can interfere searching results\n",
        "#    missing middle word - disabled, stemming - disabled\n",
        "\n",
        "results = search_similar_phrases(\"100,000-year cycle of radiation\")\n",
        "print_results(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6jFZCtb9fhK",
        "outputId": "84d62262-541b-4ea6-a506-bb19c92effbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "15361.txt, line 92, word 8: Kuhle explains the interglacial periods by the 100,000-year cycle of radiation changes due to variations in Earth's orbit. This comparatively insignificant warming, when combined with the lowering of the Nordic inland ice areas and Tibet due to the weight of the superimposed ice-load, has led to the repeated complete thawing of the inland ice areas.\n",
            "\n",
            "Found 1 results in 0.00015 seconds.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = search_similar_phrases(\"100,000 year cycle of radiation\")\n",
        "print_results(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAHmWjwv_gtH",
        "outputId": "688752e5-3c83-49c8-861c-ba8d4c77a371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Not found! 0.00050 seconds spent.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.1. It's also possible to find not exactly what we wanted due to punctuation removal\n",
        "#    missing middle word - disabled, stemming - disabled\n",
        "\n",
        "results = search_similar_phrases(\"10.74 km/h\")\n",
        "print_results(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lp71teen_rIP",
        "outputId": "a99b0fa7-7dd6-4a85-f8d6-029b670189c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "56824.txt, line 66, word 9: The maximum wind speed record in Accra is 107.4 km/h (58 knots). Strong winds associated with thunderstorm activity often cause damage to property by removing roofing material. Several areas of Accra experience microclimatic effects. Low-profile drainage basins with a north-south orientation are not as well ventilated as those oriented east-west.\n",
            "\n",
            "Found 1 results in 0.00012 seconds.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Searching for example for stop-words can lead to very poor performance due to a very big set of intersection of that stop-words in the same line\n",
        "#    missing middle word - disabled, stemming - disabled\n",
        "\n",
        "results = search_similar_phrases(\"at the a is\")\n",
        "print_results(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIUKyErBBSUc",
        "outputId": "0b35f40d-6156-4b4a-8067-2ff71fad1ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Not found! 5.71506 seconds spent.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. If using synonyms in the searching phrase, result cannot be found\n",
        "#    missing middle word - disabled, stemming - disabled\n",
        "\n",
        "results = search_similar_phrases(\"direct connection between these can be proven\")\n",
        "print_results(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ziOZS1IEAbG",
        "outputId": "4d69b416-857c-47c6-ceaa-b09157d73621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "15236.txt, line 52, word 14: Following the 2013 NSA spying scandal, ICANN endorsed the Montevideo Statement, although no direct connection between these can be proven.\n",
            "\n",
            "Found 1 results in 0.00046 seconds.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = search_similar_phrases(\"direct association between these can be proven\")\n",
        "print_results(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sq8H-14hEL8-",
        "outputId": "dd46b33f-77e8-4da0-d8b7-175a64210bae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Not found! 0.00079 seconds spent.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}